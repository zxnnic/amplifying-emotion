<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Amplifying Emotions</title>

    <!--CSS and Bootstrap-->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css">

    <!--JQuery, Popper, Bootstrap-->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
    
    <!--Custom JS files-->
    <script src="index.js"></script>
  </head>
  <body>
    <div class="container">
        <div id="title">
            <h2 class="mt-5 mb-2 text-center">
                Make it Happier! Discretizing and Amplifying Happiness in Animated Faces
            </h2>
            <div class="text-center" id="subtitle">
                <b>Jessica Bo and Zixin Zhao</b>
                <br />
                CSC2521 Course Project
            </div>
            <div class="d-flex justify-content-center my-4">
                <img src="./media/p2_goodday_valleygirl.gif" alt="participant smiling gif" style="width:800px;">
            </div>
        </div>
        
        <div id="intro">
            <h4 class="mt-5 mb-2">Introduction</h4>
            <div>
                Modeling human expression within facial animation is a complex task as it involves the interplay between many facial muscles. 
                The Facial Action Coding System (FACS) (Ekman & Friesen, 1978) has become a popular tool for encoding facial movements based on 
                the musculoskeletal system. It offers a systematic approach to model emotions on human faces. Despite the advances in automated 
                facial animation, there still needs to be more reliance on human actors and animators to create realistic expressions on 
                animated faces. Prior literature shows six main emotions (happy, sad, angry, disgust, surprise, and fear) (Ekman & Friesen, 1978), 
                and we focused our works on happiness after some pilot testing. Yu et al. have shown that people can perceive emotional expression 
                at three intensities, where each intensity is equally spaced apart. However, emotions are often expressed nonlinearly, so using 
                equally spaced apart emotions may not capture the different discrete levels of emotion expression. Thus, we want to explore the 
                possibility of determining the relative spacing between emotion intensity levels. Our project aims to focus on two main research 
                questions:
                <ol>
                    <li>
                        Can we identify the quantifiable levels of expressed happiness and spacing between each level on human faces?
                    </li>
                    <li>
                        Can we create realistic animations by artificially amplifying happiness on different facial models to the desired levels?
                    </li>
                </ol>
                
                Our project involves two main components; first, we conduct an exploratory study on the expression of happiness to examine the 
                generated FACS curves. In the second part, we build a pipeline which uses models like 
                <a href="https://developer.apple.com/documentation/arkit/arkit_in_ios/content_anchors/tracking_and_visualizing_faces" target="_blank">Apple Face</a> and 
                JALI's ValleyGirl (Edwards et al., 2016) to manipulate the levels of happiness expressed in videos. The code for this project is available on 
                <a href="https://github.com/zxnnic/amplifying_emotion" target="_blank">GitHub</a>. 
            </div>
        </div>
        <div id="bg-works">
            <h4 class="mt-5 mb-2">Related Works</h4>
            <h5>Systematic Mapping of Human Expressions using FACS</h5>
            <div class="mb-4">
                The two most popular emotion description methods are FACS and continuous models using affect dimensions. FACS helps encode facial movement based on facial muscles and can be used to reliably model spontaneous emotions (Sayette et al., 2001). Additionally, prior works have been conducted to determine which action units (AU) are correlated to the expression of the six basic emotions. A summary of their results can be seen in Table 1. Using the AU mappings, Spencer-Smith et al. explored creating a three-dimensional parameterized facial expression model. There have also been works in the automatic identification of facial expressions using FACS, with early models using hidden Markov models (Lien et al., 1998) and more recent ones using SDM and CNN-based models (Li et al., 2022).  
                <div class="d-flex justify-content-center">
                    <img src="./media/table1_facs.jpg" style="width:500px;">
                    <p>Table 1</p>
                </div>
                
            </div>
            <h5>Detecting and Quantifying Emotion Expressions</h5>
            <div>
                Examining human perception of facial expressions of emotion has been a research topic in psychophysics for some time. Within psychophysics, Marneweck et al. conducted studies to examine the threshold for distinguishing between different levels of emotions for four basic emotions: anger, disgust, happiness, and sadness. They gathered images of actors making a neutral face and one for the four basic emotions tested at maximum intensity. Then morphed the neutral with the one with maximum emotion intensity at 5% increments and showed each to participants to determine whether there were differences between the intensified images. They found that participants could only tell when images had 10-15% differences between them. Additionally, there were differences between each emotion, with happiness requiring the least difference between the two images to tell apart and sadness requiring the largest differences.
                <br  /><br  />
                Amaya et al. explored applying “emotion transforms” with respect to speed and range of movement to generate emotions on animated faces. They considered emotion a secondary movement that augmented the primary body movement and computed the difference between the emotional and neutral movements. They used an extracted emotional curve as a mask to transform the phase and amplitudes of neutral movements. Their work focused more on bodily movement. However, we draw inspiration from their methods of extracting an “emotion curve” to apply to a neutral facial animation. 
                <br  /><br  />
                Another work we draw inspiration from and closely relate to our project is one by Yu et al., which focuses on synthesizing facial expressions without using human actors. Using FACS-certified actors, they built three-dimensional models of the FACS codes using the randomly sampled points from the AU performance data captured and linearly combined them to formulate a base emotion model for each of the six emotions. They found the most important AUs for each emotion through a human perception experiment, which we utilize as a starting point for our AU data extraction. Through mapping of the AUs, they show that the activations of each AU occur at different times and have different rates of growth, thus showing that FACS curves for emotions are nonlinear. Following the AU extraction, they validated their AU selection by generating three equally spaced intensity levels for each emotion and had participants determine the more intense one. Participants received a 95.4% accuracy. 
                <br  /><br  />
                As we are working with amplification of emotions on human faces, we have to be aware of the level of exaggeration to avoid the uncanny valley. There are also differences in the perception of realistic human faces and less realistic ones. In general, less realistic faces require more exaggerated faces to reach the emotional intensity that a more realistic human face shows (Mäkäräinen et al., 2014). When comparing perceived realism and eeriness of animated faces, shape is the dominant factor for rating realism and expression intensity (Zell et al., 2015). Furthermore, Zell et al. compared the effect of realism levels in skin texture maps, stylization, shading, lighting, and texture in animated faces. They found that realism is not always preferred and is not a good predictor for animation appeal, eeriness, or attractiveness. One of their experiments found that the models with the highest appeal are closer to reality but still maintain some stylization. There are also different portions of the face that humans tend to use to determine the emotion of animated faces; for instance, for happiness, the most important features in descending order are mouth, eyebrows, and eyes (Zhang et al., 2021). Moreover, when comparing the expression intensity of the animated faces, the more intense happy faces were often perceived as less happy overall (Zhang et al., 2021).    
            </div>
            
        </div>
        <div id="extraction">
            <h4 class="mt-5 mb-2">Facial Landmark Extraction and AU Identification</h4>
            <div>
                For this project, we are using existing facial mapping technology (i.e. Apple’s ARKit), as facial landmark detection is a nontrivial task. Prior works (Sayette et al., 2001; Spencer-Smith et al., 2001; Yu et al., 2013) have performed perceptual studies to determine which AU activation induces the perception of each of the six basic emotions. While the literature identifies relevant AUs to target, they do not specify the value for activation or define the path of movement of each AUs. There are some FACS curved shown in Yu et al., however, from our review of literature, there does not seem to be a standard curve for “happiness”. Additionally, there is no one-to-one mapping of blendshapes available from ARKit outputs and FACS. Therefore, we had to create mapping of blendshapes from ARKit to FACS using Chris Landreth’s lecture slides (Landreth, 2023), open-source resources and manual mapping to serve as a foundation to subsequent analysis and emotion amplification. Our results for Anger and Happiness can be seen in Table 2 and 3. 
                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/table2_anger.jpg" style="width:550px;">
                    <p>Table 2</p>
                </div>

                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/table3_happy.jpg" style="width:550px;">
                    <p>Table 3</p>
                </div>
                
                Using the tables we manually created animations of ValleyGirl displaying varying levels of anger and happiness. Figure 1 shows ValleyGirl and Apple Face at varying intensity of anger.
                <div class="my-3 row d-flex justify-content-center">
                    <img src="./media/pilot_anger.png" style="width:800px;">
                </div>
                Similarly, Figure 2 shows ValleyGirl and Apple Face displaying different levels of happiness and the FACS curves.
                <div class="my-3 row d-flex justify-content-center">
                    <img src="./media/pilot_smile.png" style="width:800px;">
                </div>
                Using ValleyGirl, we are limited in replicating reality from our own perception of expressions of anger and happiness. Therefore, we had one of our researchers (Zixin) perform a smiling motion and captured it using ARKit. Using the generated AU curves, we found that there were key points related to the different quantifiable levels of happiness and each level had varying durations. We found that in Zixin’s smile video, there are six main stages of happiness:
                <p class="text-center fw-bold">
                    Neutral - Slight Smile - Smile - Genuine Smile - Laughter
                </p>
                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/nicole_smile_graph.png" style="width:700px;">
                </div>
                In Figure 3, we plotted all the main identified AUs on the left side, as the right side is assumed to be mirrored, that are related to expressions of happiness from previous works. We observed that there are nonlinear activations of different AUs, with MouthSmileLeft being activated first and having almost a stepladder shape. The shape of the curves presented here shows that the duration of each level of happiness is not evenly spaced, therefore varying intensity using equally spaced intervals is likely not going to be able to capture the different expressions realistically. 
            </div>
        </div>
        <div id="recording-study">
            <h4 class="mt-5 mb-2">Recording Study</h4>
            <div>
                To collect initial data on the cognitive process of expressing emotions, we contacted drama clubs and theatre groups across campus. We hoped to gain insight into the various ways people express emotions consciously through acting. However, due to low participant turnout, we conducted a separate data collection study using available participants. We gathered a total of eight participants (N=8) for our recording study. Our data collection study consisted of three main parts:
                
                <ol>
                    <li>Semi-structured interview about different ways emotions can be expressed</li>
                    <li>Performing neutral to full intensity happy and neutral to full intensity angry faces</li>
                    <li> Reciting two sentences in two different tones (neutral-happy and neutral-angry)</li>
                </ol>
                For the two sentences, we chose the following:
                <ul>
                    <li>I had such a good day today.</li>
                    <li>I almost got run over by a car today. </li>
                </ul>
            </div>
            <h5>Interview Results</h5>
            <div class="mb-4">
                We conducted our study in various settings, however the same questions were asked to each participant and the order was unchanged. From the interviews we found that people generally perceived anger having three separate levels: annoyed, angry, and yelling. Similar facial muscles were engaged, but with each increased level, the activation of the muscles would increase. For happiness, people found that there were four different expressions people make at varying levels of happiness: small smile, cheeks puffed smile, eyes crinkles smile, laughing with teeth. In addition to facial expressions, participants would bring up nuances of expressing emotions that cannot be captured by face alone, such as a conflicting body language and face expressions as well as body movements as a facet of expressing emotions like anger.
            </div>
            <h5>Data Collection Results Limitations</h5>
            <div class="mb-4">
                There were limitations due to the lack of one-to-one mapping of Apple Face and ValleyGirl blenshapes, making it initially difficult to transfer the emotions recorded using ARKit onto ValleyGirl. The differences in the neutral values needed to be manually hardcoded. Below is a video explaining the problems we encountered during initial testing anger expressions. Due to the problems that arose with unnatural looking angry expressions during the participant study. We decided to not use the data collected for anger. 
                Instead our project would focus on expressions of happiness.
                <div class="my-3 d-flex justify-content-center">
                    <video width="600px" controls>
                        <source src="./media/p3_problems_w_mouth.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                      </video>
                </div>
                Additionally, we encountered some problems with head tracking in ARKit and capturing FACS curves when people are laughing. As laughing often involves movements of the head and body, the captured curves look chaotic and the mapped expressions does not mirror the expression of the human actor perfectly. For example, Figure 4 shows a participant's correlated FACS curve showing the activated AUs when they are laughing. Then Figure 5 shows the captured video and modelled facial expressions that was obtained through ARKit. 
                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/p2_laughing_facs.png" style="width:400px;">
                </div>
                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/p2_gif.gif" style="width:200px;"><img src="./media/p2_happy_appleface.gif" style="width:200px;"><img src="./media/p2_happy_jali.gif" style="width:200px;">
                </div>
            </div>
            <h5>Emotion Level Discretization</h5>
            <div class="mb-4">
                Using the previously identified AUs for happiness, as noted in Table 2, we extracted all the recorded blendshape weights of each participants to determine whether we would be able to obtain a generalized graph of a happiness FACS curve. Figure 6 shows the FACS curve of each identified AU for happiness of all the participants. 
                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/facs_curves_happy.png" style="width:900px;">
                </div>
                After obtaining these curves, we resampled them to the same length and start activation time, so that we can approximate a mean curve for all identified AUs.
                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/mean_aus_happy.png" style="width:900px;">
                </div>

                <div class="my-3 d-flex justify-content-center">
                    <img src="./media/table4_pcafeatures.jpg" style="width:700px;">
                </div>
            </div>

            <h5>Emotion Level Discretization</h5>
            <div class="mb-4">

            </div>
        </div>


        <div id="perc-study">
            <h4 class="mt-5 mb-2">Perceptual Study</h4>
            <h5>Results</h5>
            <div class="mb-4">
                
            </div>
        </div>
        <div id="discussion">
            <h4 class="mt-5 mb-2">Discussion</h4>
            insights
            future work
        </div>
        <div id="conclusion">
            <h4 class="mt-5 mb-2">Conclusion</h4>
        </div>
        <div id="ref">
            <h4 class="mt-5 mb-2">Reference</h4>
            <ol>
                <li>Ekman, P., & Friesen, W. V. (1978). Facial Action Coding System (FACS) [Database record]. APA PsycTests. DOI: 10.1037/t27734-000 </li>
                <li>Yu, H., Garrod, O. G., & Schyns, P. G. (2012). Perception-driven facial expression synthesis. Computers & Graphics, 36(3), 152-162.</li>
                <li>Sayette, M. A., Cohn, J. F., Wertz, J. M., Perrott, M. A., & Parrott, D. J. (2001). A psychometric evaluation of the facial action coding system for assessing spontaneous expression. Journal of nonverbal behavior, 25, 167-185.</li>
                <li>Spencer-Smith, J., Wild, H., Innes-Ker, Å. H., Townsend, J., Duffy, C., Edwards, C., & Pair, J. W. (2001). Making faces: Creating three-dimensional parameterized models of facial expression. Behavior Research Methods, Instruments, & Computers, 33, 115-123.</li>
                <li>Amaya, K., Bruderlin, A. and Calvert, T. (1996) Emotion from motion. Graphics interface. Vol. 96. 222-229.</li>
                <li>Marneweck, M., Loftus, A. and Hammond, G. (2013). Psychophysical measures of sensitivity to facial expression of emotion. Frontiers in Psychology, 4, 63.</li>
                <li>Lien, J. J., Kanade, T., Cohn, J. F.  and Li, C. (1998) Automated facial expression recognition based on FACS action units. Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition, Nara, Japan, 390-395.</li>
                <li>Li, S. and Deng, W. (2020). Deep facial expression recognition: A survey. IEEE transactions on affective computing, 13(3), 1195-1215.</li>
                <li>Mäkäräinen, M., Kätsyri, J. and Takala, T. (2014). Exaggerating facial expressions: A way to intensify emotion or a way to the uncanny valley?. Cognitive Computation, 6, 708-721.</li>
                <li>Zhang, S., Liu, X., Yang, X., Shu, Y., Liu, N., Zhang, D. and Liu, Y.J. (2021). The influence of key facial features on recognition of emotion in cartoon faces. Frontiers in psychology, 12, p.687974.</li>
                <li>Zell, E., Aliaga, C., Jarabo, A., Zibrek, K., Gutierrez, D., McDonnell, R. and Botsch, M. (2015). To stylize or not to stylize? The effect of shape and material stylization on the perception of computer-generated faces. ACM Transactions on Graphics (TOG), 34(6), pp.1-12.</li>
                <li>Edwards, P., Landreth, C., Fiume, E. and Singh, K. (2016). Jali: an animator-centric viseme model for expressive lip synchronization. ACM Transactions on graphics (TOG), 35(4), pp.1-11.</li>
                <li>Landreth, C. (2023). Making Faces. GDC Animation Summit. Presentation.</li>

            </ol>
        </div>
    </div>
  </body>
</html>